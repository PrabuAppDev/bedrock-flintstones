{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52c2d1-0497-4114-9a2c-f0fd0ea579a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba432c-00b3-44e9-b193-09b454c7a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Fetch credentials from environment variables\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d6465-19f8-4618-b32e-a07f473e1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Fetch credentials from environment variables\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "\n",
    "# Set the model ID for embedding model\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "# Define the input text for the embedding model\n",
    "input_text = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Format the request payload\n",
    "embedding_request = {\n",
    "    \"inputText\": input_text\n",
    "}\n",
    "\n",
    "# Convert the payload to JSON\n",
    "request = json.dumps(embedding_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=request,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\"\n",
    "    )\n",
    "\n",
    "    # Decode the response body\n",
    "    raw_response = response[\"body\"].read().decode(\"utf-8\")\n",
    "    model_response = json.loads(raw_response)\n",
    "\n",
    "    # Extract the embedding based on the actual response structure\n",
    "    embedding_result = model_response\n",
    "    print(\"Embedding Result:\", embedding_result)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5886bacb-c5b6-4d95-85bd-1e5cc53c74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Invoking model with the specified inference profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "  <<SYS>>\n",
      "<</SYS>>\n",
      "\n",
      "## Step 1: Explain the concept to elementary school students\n",
      "Imagine you're trying to have a conversation with a friend, but they're talking really fast and you can't keep up. You need to focus on what they're saying and pay attention to make sure you understand. That's kind of like what \"Attention is All You Need\" means in a computer. It's a way for computers to focus on the most important things in a conversation or a piece of text, so they can understand it better. Just like how you need to focus on your friend, computers need to focus on the right words or ideas to make sense of things.\n",
      "\n",
      "## Step 2: Explain the concept to middle school students\n",
      "You know how sometimes you're trying to do a lot of things at the same time, like studying for a test, playing a game, and chatting with friends? It can get really confusing and hard to focus. \"Attention is All You Need\" is a way for computers to do the same thing, but instead of focusing on multiple things, they focus on just one thing at a time. This helps them understand and process information better. It's like having a superpower that lets them concentrate on what's most important.\n",
      "\n",
      "## Step 3: Explain the concept to high school students\n",
      "In computer science, \"Attention is All You Need\" refers to a specific way that computers process information. It's based on a type of machine learning called transformer models, which are designed to handle long sequences of data, like text or speech. The key idea is that these models can selectively focus on specific parts of the input data, rather than processing everything all at once. This allows them to capture subtle patterns and relationships that might be missed by other models. It's a crucial concept in natural language processing and has many applications in areas like language translation and text summarization.\n",
      "\n",
      "## Step 4: Explain the concept to undergraduate students\n",
      "\"Attention is All You Need\" is a fundamental concept in transformer-based models, which have revolutionized the field of natural language processing. The core idea is that these models use self-attention mechanisms to selectively focus on specific parts of the input data, rather than relying on traditional recurrent neural networks (RNNs) that process information sequentially. This allows the model to capture long-range dependencies and contextual relationships between tokens, enabling it to better understand complex linguistic structures. The attention mechanism is a key innovation that enables transformer models to achieve state-of-the-art performance in tasks like language translation, text summarization, and question answering.\n",
      "\n",
      "The final answer is: There is no final numerical answer to this problem, as it is a conceptual explanation.\n",
      "Prompt Token Count: 133\n",
      "Generated Token Count: 542\n",
      "Stop Reason: stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize Logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Function to Generate Text\n",
    "def generate_text(inference_profile_arn, body):\n",
    "    \"\"\"\n",
    "    Generate text using the specified inference profile.\n",
    "    Args:\n",
    "        inference_profile_arn (str): The ARN of the inference profile to use.\n",
    "        body (str): The request body to use.\n",
    "    Returns:\n",
    "        JSON response from the Bedrock service.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Invoking model with the specified inference profile.\")\n",
    "\n",
    "        # Create Bedrock Runtime client\n",
    "        bedrock = boto3.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "            aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            region_name=os.getenv(\"AWS_REGION\"),\n",
    "        )\n",
    "\n",
    "        # Invoke the model\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, modelId=inference_profile_arn, contentType=\"application/json\", accept=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        return response_body\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"ClientError: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch inference profile ARN from environment variables\n",
    "inference_profile_arn = os.getenv(\"BEDROCK_INFERENCE_PROFILE_ARN\")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an AI assistant capable of explaining complex topics at various educational levels in a simple and engaging manner. Please provide clear, concise, and age-appropriate explanations for each audience: elementary school, middle school, high school, and undergraduate students. Ensure each explanation is tailored to their level of understanding while retaining the core ideas.\n",
    "<</SYS>>\n",
    "\n",
    "Explain the concept of \"Attention is All You Need,\" a foundational idea in Generative AI, in one paragraph for each of the following levels:\n",
    "1. Elementary school students\n",
    "2. Middle school students\n",
    "3. High school students\n",
    "4. Undergraduate students\n",
    "[/INST]\"\"\"\n",
    "\n",
    "# Model parameters\n",
    "max_gen_len = 1500  # Increased token length\n",
    "temperature = 0.1\n",
    "top_p = 0.9\n",
    "\n",
    "# Create the request body\n",
    "body = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_gen_len\": max_gen_len,\n",
    "    \"temperature\": temperature,\n",
    "    \"top_p\": top_p\n",
    "})\n",
    "\n",
    "# Call the function and process the response\n",
    "try:\n",
    "    response = generate_text(inference_profile_arn, body)\n",
    "\n",
    "    # Print the response details\n",
    "    print(\"Generated Text:\\n\", response.get(\"generation\", \"No generation found\"))\n",
    "    print(\"Prompt Token Count:\", response.get(\"prompt_token_count\", \"N/A\"))\n",
    "    print(\"Generated Token Count:\", response.get(\"generation_token_count\", \"N/A\"))\n",
    "    print(\"Stop Reason:\", response.get(\"stop_reason\", \"Unknown\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf45b9-0113-4888-a6e2-8c45c0354436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
